{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des bibliothèques nécessaires\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, make_scorer, mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_power               float64\n",
      "weight_min              float64\n",
      "weight_max              float64\n",
      "urb_cons                float64\n",
      "exturb_cons             float64\n",
      "                         ...   \n",
      "range_LUXE              float64\n",
      "range_MOY-INFER         float64\n",
      "range_MOY-INFERIEURE    float64\n",
      "range_MOY-SUPER         float64\n",
      "range_SUPERIEURE        float64\n",
      "Length: 3919, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Chargement des données\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Sélection des features\n",
    "numeric_features = ['max_power', 'weight_min', 'weight_max', 'urb_cons', \n",
    "                    'exturb_cons', 'overall_cons', 'co', 'hc', 'nox', 'hcnox', 'ptcl']\n",
    "categorical_features = ['brand', 'model', 'car_class', 'fuel_type', 'hybrid', 'grbx_type_ratios', 'range']\n",
    "\n",
    "# Créer une copie pour éviter de modifier les données originales\n",
    "X_train = train_df.copy()\n",
    "X_test = test_df.copy()\n",
    "\n",
    "\n",
    "# 1. Gestion des valeurs manquantes et standardisation des variables numériques\n",
    "for col in numeric_features:\n",
    "    combined_values = pd.concat([X_train[col], X_test[col]])\n",
    "    mean_value = combined_values.mean()\n",
    "    std_value = combined_values.std()\n",
    "    \n",
    "    X_train[col] = X_train[col].fillna(mean_value)\n",
    "    X_test[col] = X_test[col].fillna(mean_value)\n",
    "    \n",
    "    X_train[col] = (X_train[col] - mean_value) / std_value\n",
    "    X_test[col] = (X_test[col] - mean_value) / std_value\n",
    "\n",
    "# Standardisation de co2 et création de y_train\n",
    "mean_value = X_train['co2'].mean()\n",
    "std_value = X_train['co2'].std()\n",
    "X_train['co2'] = (X_train['co2'] - mean_value) / std_value\n",
    "y_train = X_train.pop('co2')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2. Encodage des variables catégoriques avec OneHotEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Initialisation de l'encodeur\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')  # sparse_output=False pour obtenir un array dense\n",
    "\n",
    "# Combiner les données d'entraînement et de test pour fit l'encodeur\n",
    "combined_cat = pd.concat([X_train[categorical_features], X_test[categorical_features]], axis=0)\n",
    "ohe.fit(combined_cat)\n",
    "\n",
    "# Transformer les données d'entraînement\n",
    "X_train_ohe = ohe.transform(X_train[categorical_features])\n",
    "X_train_ohe_df = pd.DataFrame(X_train_ohe, columns=ohe.get_feature_names_out(categorical_features), index=X_train.index)\n",
    "\n",
    "# Transformer les données de test\n",
    "X_test_ohe = ohe.transform(X_test[categorical_features])\n",
    "X_test_ohe_df = pd.DataFrame(X_test_ohe, columns=ohe.get_feature_names_out(categorical_features), index=X_test.index)\n",
    "\n",
    "# Remplacer les colonnes catégoriques par leurs versions encodées\n",
    "X_train = pd.concat([X_train[numeric_features], X_train_ohe_df], axis=1)\n",
    "X_test = pd.concat([X_test[numeric_features], X_test_ohe_df], axis=1)\n",
    "\n",
    "# Vérification des types\n",
    "print(X_train.dtypes)  # Assure-toi que tout est numérique (float64, int64, etc.)\n",
    "\n",
    "# Conversion en tenseurs PyTorch (exemple)\n",
    "class CO2Dataset:\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = torch.tensor(X.values, dtype=torch.float32)\n",
    "        if y is not None:\n",
    "            self.y = torch.tensor(y.values, dtype=torch.float32).reshape(-1, 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train_nn, X_val, y_train_nn, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.1, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "LR = 3e-4\n",
    "WEIGHT_DECAY = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilisation de: cpu\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Réseau de neurones avec PyTorch pour la régression:\n",
    "\"\"\"\n",
    "\n",
    "# Vérifier si CUDA est disponible\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Utilisation de: {device}\")\n",
    "\n",
    "# Créer un dataset PyTorch personnalisé\n",
    "class CO2Dataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = torch.tensor(X.values, dtype=torch.float32)\n",
    "        \n",
    "        if y is not None:\n",
    "            self.y = torch.tensor(y.values, dtype=torch.float32).reshape(-1, 1)\n",
    "            self.has_target = True\n",
    "        else:\n",
    "            self.has_target = False\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.has_target:\n",
    "            return self.X[idx], self.y[idx]\n",
    "        else:\n",
    "            return self.X[idx]\n",
    "\n",
    "# Définition du modèle de réseau de neurones\n",
    "class ImprovedCO2RegressionNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[128, 64, 32], dropout_rates=[0.3, 0.2, 0.1]):\n",
    "        super(ImprovedCO2RegressionNet, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        in_features = input_dim\n",
    "        \n",
    "        # Construction dynamique des couches cachées\n",
    "        for i, hidden_dim in enumerate(hidden_dims):\n",
    "            layers.append(nn.Linear(in_features, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            \n",
    "            # Appliquer dropout sauf pour la dernière couche cachée\n",
    "            if i < len(dropout_rates):\n",
    "                layers.append(nn.Dropout(dropout_rates[i]))\n",
    "            \n",
    "            in_features = hidden_dim\n",
    "        \n",
    "        # Couche de sortie\n",
    "        layers.append(nn.Linear(in_features, 1))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "\n",
    "# Création des datasets et dataloaders\n",
    "train_dataset = CO2Dataset(X_train_nn, y_train_nn)\n",
    "val_dataset = CO2Dataset(X_val, y_val)\n",
    "test_dataset = CO2Dataset(X_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ImprovedCO2RegressionNet:\n\tsize mismatch for model.0.weight: copying a param with shape torch.Size([256, 19]) from checkpoint, the shape in current model is torch.Size([256, 3919]).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      3\u001b[39m model = ImprovedCO2RegressionNet(\n\u001b[32m      4\u001b[39m     input_dim, \n\u001b[32m      5\u001b[39m     hidden_dims=[\u001b[32m256\u001b[39m, \u001b[32m128\u001b[39m, \u001b[32m64\u001b[39m],  \u001b[38;5;66;03m# Couches plus larges\u001b[39;00m\n\u001b[32m      6\u001b[39m     dropout_rates=[\u001b[32m0.4\u001b[39m, \u001b[32m0.3\u001b[39m, \u001b[32m0.2\u001b[39m]  \u001b[38;5;66;03m# Dropout plus agressif\u001b[39;00m\n\u001b[32m      7\u001b[39m ).to(device)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# Chargement des poids sauvegardés\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mco2_regression_model.pth\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[32m     14\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPas de modèle sauvegardé trouvé, initialisation d\u001b[39m\u001b[33m'\u001b[39m\u001b[33mun nouveau modèle.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/INSA/4IF/ML/4IF_ML_TP1/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2581\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2573\u001b[39m         error_msgs.insert(\n\u001b[32m   2574\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2575\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2576\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2577\u001b[39m             ),\n\u001b[32m   2578\u001b[39m         )\n\u001b[32m   2580\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2581\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2582\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2583\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2584\u001b[39m         )\n\u001b[32m   2585\u001b[39m     )\n\u001b[32m   2586\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for ImprovedCO2RegressionNet:\n\tsize mismatch for model.0.weight: copying a param with shape torch.Size([256, 19]) from checkpoint, the shape in current model is torch.Size([256, 3919])."
     ]
    }
   ],
   "source": [
    "# Initialisation du modèle, fonction de perte et optimiseur\n",
    "input_dim = X_train.shape[1]\n",
    "model = ImprovedCO2RegressionNet(\n",
    "    input_dim, \n",
    "    hidden_dims=[256, 128, 64],  # Couches plus larges\n",
    "    dropout_rates=[0.4, 0.3, 0.2]  # Dropout plus agressif\n",
    ").to(device)\n",
    "\n",
    "try:\n",
    "    # Chargement des poids sauvegardés\n",
    "    model.load_state_dict(torch.load('co2_regression_model.pth'))\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Pas de modèle sauvegardé trouvé, initialisation d'un nouveau modèle.\")\n",
    "\n",
    "criterion = nn.MSELoss()  # Mean Squared Error pour la régression\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction d'entraînement\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Mode entraînement\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            # Remise à zéro des gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            \n",
    "            # Backward pass et optimisation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * X_batch.size(0)\n",
    "        \n",
    "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        \n",
    "        # Mode évaluation\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                \n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                \n",
    "                running_loss += loss.item() * X_batch.size(0)\n",
    "        \n",
    "        epoch_val_loss = running_loss / len(val_loader.dataset)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        \n",
    "        # # Ajustement du learning rate\n",
    "        # scheduler.step(epoch_val_loss)\n",
    "        \n",
    "        # Affichage des métriques\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}')\n",
    "        \n",
    "        # Sauvegarde du meilleur modèle\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "    \n",
    "    # Chargement du meilleur modèle\n",
    "    model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 1.0079, Val Loss: 0.8474\n",
      "Epoch 2/50, Train Loss: 0.9270, Val Loss: 0.8284\n",
      "Epoch 3/50, Train Loss: 0.7248, Val Loss: 1.3569\n",
      "Epoch 4/50, Train Loss: 0.4923, Val Loss: 0.9168\n",
      "Epoch 5/50, Train Loss: 0.4006, Val Loss: 0.3615\n",
      "Epoch 6/50, Train Loss: 0.3436, Val Loss: 3.5706\n",
      "Epoch 7/50, Train Loss: 0.3061, Val Loss: 0.6187\n",
      "Epoch 8/50, Train Loss: 0.2692, Val Loss: 0.3626\n",
      "Epoch 9/50, Train Loss: 0.2524, Val Loss: 0.3822\n",
      "Epoch 10/50, Train Loss: 0.2385, Val Loss: 0.4963\n",
      "Epoch 11/50, Train Loss: 0.2242, Val Loss: 0.8814\n",
      "Epoch 12/50, Train Loss: 0.2139, Val Loss: 0.7698\n",
      "Epoch 13/50, Train Loss: 0.2053, Val Loss: 0.7588\n",
      "Epoch 14/50, Train Loss: 0.2158, Val Loss: 6.6979\n",
      "Epoch 15/50, Train Loss: 0.2797, Val Loss: 6.9334\n",
      "Epoch 16/50, Train Loss: 0.2011, Val Loss: 3.8406\n",
      "Epoch 17/50, Train Loss: 0.1915, Val Loss: 5.9363\n",
      "Epoch 18/50, Train Loss: 0.1789, Val Loss: 5.5623\n",
      "Epoch 19/50, Train Loss: 0.1810, Val Loss: 8.6932\n",
      "Epoch 20/50, Train Loss: 0.1729, Val Loss: 3.8459\n",
      "Epoch 21/50, Train Loss: 0.1775, Val Loss: 3.1271\n",
      "Epoch 22/50, Train Loss: 0.1594, Val Loss: 1.3263\n",
      "Epoch 23/50, Train Loss: 0.1593, Val Loss: 3.0705\n",
      "Epoch 24/50, Train Loss: 0.1551, Val Loss: 1.2461\n",
      "Epoch 25/50, Train Loss: 0.1525, Val Loss: 8.3570\n",
      "Epoch 26/50, Train Loss: 0.1446, Val Loss: 20.7763\n",
      "Epoch 27/50, Train Loss: 0.1413, Val Loss: 1.5297\n",
      "Epoch 28/50, Train Loss: 0.1428, Val Loss: 1.0723\n",
      "Epoch 29/50, Train Loss: 0.1412, Val Loss: 10.5123\n",
      "Epoch 30/50, Train Loss: 0.1405, Val Loss: 24.7371\n",
      "Epoch 31/50, Train Loss: 0.1283, Val Loss: 0.4627\n",
      "Epoch 32/50, Train Loss: 0.1291, Val Loss: 7.0487\n",
      "Epoch 33/50, Train Loss: 0.1397, Val Loss: 2.1294\n",
      "Epoch 34/50, Train Loss: 0.1267, Val Loss: 0.9065\n",
      "Epoch 35/50, Train Loss: 0.1267, Val Loss: 8.8274\n",
      "Epoch 36/50, Train Loss: 0.1208, Val Loss: 2.0845\n",
      "Epoch 37/50, Train Loss: 0.1211, Val Loss: 1.6599\n",
      "Epoch 38/50, Train Loss: 0.1125, Val Loss: 2.9238\n",
      "Epoch 39/50, Train Loss: 0.1152, Val Loss: 3.4089\n",
      "Epoch 40/50, Train Loss: 0.1182, Val Loss: 1.2229\n",
      "Epoch 41/50, Train Loss: 0.1120, Val Loss: 1.3504\n",
      "Epoch 42/50, Train Loss: 0.1116, Val Loss: 3.2936\n",
      "Epoch 43/50, Train Loss: 0.1111, Val Loss: 2.3432\n",
      "Epoch 44/50, Train Loss: 0.1150, Val Loss: 4.2346\n",
      "Epoch 45/50, Train Loss: 0.1090, Val Loss: 2.3011\n",
      "Epoch 46/50, Train Loss: 0.1116, Val Loss: 0.7547\n",
      "Epoch 47/50, Train Loss: 0.1098, Val Loss: 0.7790\n",
      "Epoch 48/50, Train Loss: 0.1138, Val Loss: 334.8002\n",
      "Epoch 49/50, Train Loss: 0.1107, Val Loss: 1.7728\n",
      "Epoch 50/50, Train Loss: 0.1112, Val Loss: 0.6713\n",
      "Modèle sauvegardé dans 'co2_regression_model.pth'\n",
      "Validation MAE: 0.3991\n",
      "Validation R²: 0.2843\n"
     ]
    }
   ],
   "source": [
    "# Entraînement du modèle\n",
    "trained_model, train_losses, val_losses = train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, num_epochs=NUM_EPOCHS\n",
    ")\n",
    "\n",
    "torch.save(trained_model.state_dict(), 'co2_regression_model.pth')\n",
    "print(\"Modèle sauvegardé dans 'co2_regression_model.pth'\")\n",
    "\n",
    "# Graphique des pertes\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Courbes d\\'apprentissage')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('nn_training_loss.png')\n",
    "plt.close()\n",
    "\n",
    "# Évaluation sur l'ensemble de validation\n",
    "model.eval()\n",
    "val_predictions = []\n",
    "val_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in val_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        val_predictions.extend(outputs.cpu().numpy())\n",
    "        val_targets.extend(y_batch.numpy())\n",
    "\n",
    "val_predictions = np.array(val_predictions).flatten()\n",
    "val_targets = np.array(val_targets).flatten()\n",
    "\n",
    "val_mae = mean_absolute_error(val_targets, val_predictions)\n",
    "val_r2 = r2_score(val_targets, val_predictions)\n",
    "\n",
    "print(f\"Validation MAE: {val_mae:.4f}\")\n",
    "print(f\"Validation R²: {val_r2:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Prédictions sur l'ensemble de test\n",
    "model.eval()\n",
    "test_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        test_predictions.extend(outputs.cpu().numpy())\n",
    "\n",
    "Y_test_nn = np.array(test_predictions).flatten()\n",
    "\n",
    "# Sauvegarde des résultats\n",
    "resultat = [(test_df['id'].iloc[i], Y_test_nn[i]) for i in range(len(Y_test_nn))]\n",
    "with open('resultat_pytorch.csv', 'w') as f:\n",
    "    f.write(\"id,co2\\n\")\n",
    "    for id, co2 in resultat:\n",
    "        f.write(f\"{id},{int(round(co2))}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
