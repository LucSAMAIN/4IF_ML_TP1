{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des bibliothèques nécessaires\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, make_scorer, mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forme des données d'entraînement finales: (41257, 103)\n",
      "Forme des données de test finales: (13753, 103)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41257 entries, 0 to 41256\n",
      "Columns: 102 entries, max_power to range_SUPERIEURE\n",
      "dtypes: float64(102)\n",
      "memory usage: 32.1 MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_power</th>\n",
       "      <th>weight_min</th>\n",
       "      <th>weight_max</th>\n",
       "      <th>urb_cons</th>\n",
       "      <th>exturb_cons</th>\n",
       "      <th>overall_cons</th>\n",
       "      <th>co</th>\n",
       "      <th>hc</th>\n",
       "      <th>nox</th>\n",
       "      <th>hcnox</th>\n",
       "      <th>...</th>\n",
       "      <th>grbx_type_ratios_S 6</th>\n",
       "      <th>grbx_type_ratios_V .</th>\n",
       "      <th>grbx_type_ratios_V 0</th>\n",
       "      <th>range_ECONOMIQUE</th>\n",
       "      <th>range_INFERIEURE</th>\n",
       "      <th>range_LUXE</th>\n",
       "      <th>range_MOY-INFER</th>\n",
       "      <th>range_MOY-INFERIEURE</th>\n",
       "      <th>range_MOY-SUPER</th>\n",
       "      <th>range_SUPERIEURE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.080623</td>\n",
       "      <td>-0.431333</td>\n",
       "      <td>-0.630533</td>\n",
       "      <td>-0.228571</td>\n",
       "      <td>-0.317796</td>\n",
       "      <td>-0.264052</td>\n",
       "      <td>-0.675846</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.168272</td>\n",
       "      <td>0.434755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.419458</td>\n",
       "      <td>0.283602</td>\n",
       "      <td>0.031296</td>\n",
       "      <td>0.299729</td>\n",
       "      <td>0.259863</td>\n",
       "      <td>0.314959</td>\n",
       "      <td>-0.710325</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.180264</td>\n",
       "      <td>-0.018698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.462095</td>\n",
       "      <td>1.645382</td>\n",
       "      <td>1.246225</td>\n",
       "      <td>1.404358</td>\n",
       "      <td>2.185393</td>\n",
       "      <td>1.834861</td>\n",
       "      <td>-0.786180</td>\n",
       "      <td>-0.651084</td>\n",
       "      <td>3.709942</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.750040</td>\n",
       "      <td>-2.800830</td>\n",
       "      <td>-2.509655</td>\n",
       "      <td>-1.525309</td>\n",
       "      <td>-2.050773</td>\n",
       "      <td>-1.783954</td>\n",
       "      <td>-0.096592</td>\n",
       "      <td>0.645676</td>\n",
       "      <td>-0.623968</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.021319</td>\n",
       "      <td>0.862358</td>\n",
       "      <td>0.255845</td>\n",
       "      <td>0.251702</td>\n",
       "      <td>0.163587</td>\n",
       "      <td>0.242583</td>\n",
       "      <td>-0.958577</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.261809</td>\n",
       "      <td>-0.872257</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   max_power  weight_min  weight_max  urb_cons  exturb_cons  overall_cons  \\\n",
       "0  -1.080623   -0.431333   -0.630533 -0.228571    -0.317796     -0.264052   \n",
       "1  -0.419458    0.283602    0.031296  0.299729     0.259863      0.314959   \n",
       "2   0.462095    1.645382    1.246225  1.404358     2.185393      1.834861   \n",
       "3  -0.750040   -2.800830   -2.509655 -1.525309    -2.050773     -1.783954   \n",
       "4   0.021319    0.862358    0.255845  0.251702     0.163587      0.242583   \n",
       "\n",
       "         co        hc       nox     hcnox  ...  grbx_type_ratios_S 6  \\\n",
       "0 -0.675846  0.000000 -0.168272  0.434755  ...                   0.0   \n",
       "1 -0.710325  0.000000 -0.180264 -0.018698  ...                   0.0   \n",
       "2 -0.786180 -0.651084  3.709942  0.000000  ...                   0.0   \n",
       "3 -0.096592  0.645676 -0.623968  0.000000  ...                   0.0   \n",
       "4 -0.958577  0.000000 -0.261809 -0.872257  ...                   0.0   \n",
       "\n",
       "   grbx_type_ratios_V .  grbx_type_ratios_V 0  range_ECONOMIQUE  \\\n",
       "0                   0.0                   0.0               0.0   \n",
       "1                   0.0                   0.0               0.0   \n",
       "2                   0.0                   0.0               0.0   \n",
       "3                   0.0                   0.0               0.0   \n",
       "4                   0.0                   0.0               0.0   \n",
       "\n",
       "   range_INFERIEURE  range_LUXE  range_MOY-INFER  range_MOY-INFERIEURE  \\\n",
       "0               0.0         0.0              1.0                   0.0   \n",
       "1               0.0         0.0              0.0                   0.0   \n",
       "2               0.0         0.0              1.0                   0.0   \n",
       "3               0.0         0.0              1.0                   0.0   \n",
       "4               0.0         0.0              1.0                   0.0   \n",
       "\n",
       "   range_MOY-SUPER  range_SUPERIEURE  \n",
       "0              0.0               0.0  \n",
       "1              1.0               0.0  \n",
       "2              0.0               0.0  \n",
       "3              0.0               0.0  \n",
       "4              0.0               0.0  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chargement des données\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Remove model:\n",
    "train_df.drop('model', axis=1, inplace=True)\n",
    "test_df.drop('model', axis=1, inplace=True)\n",
    "\n",
    "# Sélection des features\n",
    "numeric_features = ['max_power', 'weight_min', 'weight_max', 'urb_cons', \n",
    "                    'exturb_cons', 'overall_cons', 'co', 'hc', 'nox', 'hcnox', 'ptcl']\n",
    "categorical_features = ['brand', 'car_class', 'fuel_type', 'hybrid', 'grbx_type_ratios', 'range']\n",
    "\n",
    "# Créer une copie pour éviter de modifier les données originales\n",
    "X_train = train_df.copy()\n",
    "X_test = test_df.copy()\n",
    "\n",
    "\n",
    "# 1. Gestion des valeurs manquantes et standardisation des variables numériques\n",
    "for col in numeric_features:\n",
    "    combined_values = pd.concat([X_train[col], X_test[col]])\n",
    "    mean_value = combined_values.mean()\n",
    "    std_value = combined_values.std()\n",
    "    \n",
    "    X_train[col] = X_train[col].fillna(mean_value)\n",
    "    X_test[col] = X_test[col].fillna(mean_value)\n",
    "    \n",
    "    X_train[col] = (X_train[col] - mean_value) / std_value\n",
    "    X_test[col] = (X_test[col] - mean_value) / std_value\n",
    "\n",
    "# Standardisation de co2 et création de y_train\n",
    "# mean_value = X_train['co2'].mean()\n",
    "# std_value = X_train['co2'].std()\n",
    "# X_train['co2'] = (X_train['co2'] - mean_value) / std_value\n",
    "y_train = X_train.pop('co2')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2. Encodage des variables catégoriques avec OneHotEncoder\n",
    "\n",
    "# Initialisation du OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit sur les données d'entraînement uniquement\n",
    "encoder.fit(X_train[categorical_features])\n",
    "\n",
    "# Transformation des données d'entraînement et de test\n",
    "encoded_train = encoder.transform(X_train[categorical_features])\n",
    "encoded_test = encoder.transform(X_test[categorical_features])\n",
    "\n",
    "# Création de DataFrame à partir des données encodées avec les noms des catégories\n",
    "encoded_feature_names = []\n",
    "for i, feature in enumerate(categorical_features):\n",
    "    categories = encoder.categories_[i]\n",
    "    for category in categories:\n",
    "        encoded_feature_names.append(f\"{feature}_{category}\")\n",
    "    \n",
    "\n",
    "# Conversion des données encodées en DataFrame\n",
    "encoded_train_df = pd.DataFrame(encoded_train, columns=encoded_feature_names, index=X_train.index)\n",
    "encoded_test_df = pd.DataFrame(encoded_test, columns=encoded_feature_names, index=X_test.index)\n",
    "\n",
    "# Suppression des colonnes catégoriques originales et ajout des colonnes encodées\n",
    "X_train_numeric = X_train.drop(categorical_features, axis=1)\n",
    "X_test_numeric = X_test.drop(categorical_features, axis=1)\n",
    "\n",
    "# Concaténation des variables numériques et catégoriques encodées\n",
    "X_train = pd.concat([X_train_numeric, encoded_train_df], axis=1)\n",
    "X_test = pd.concat([X_test_numeric, encoded_test_df], axis=1)\n",
    "\n",
    "# Vérification du résultat\n",
    "print(f\"Forme des données d'entraînement finales: {X_train.shape}\")\n",
    "print(f\"Forme des données de test finales: {X_test.shape}\")\n",
    "\n",
    "X_train = X_train.drop('id', axis=1)\n",
    "X_test = X_test.drop('id', axis=1)\n",
    "\n",
    "print(X_train.info())\n",
    "X_train.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train_nn, X_val, y_train_nn, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.1, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_EPOCHS = 25\n",
    "BATCH_SIZE = 64\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "active_scheduler = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilisation de: cpu\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Réseau de neurones avec PyTorch pour la régression:\n",
    "\"\"\"\n",
    "\n",
    "# Vérifier si CUDA est disponible\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Utilisation de: {device}\")\n",
    "\n",
    "# Créer un dataset PyTorch personnalisé\n",
    "class CO2Dataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = torch.tensor(X.values, dtype=torch.float32)\n",
    "        \n",
    "        if y is not None:\n",
    "            self.y = torch.tensor(y.values, dtype=torch.float32).reshape(-1, 1)\n",
    "            self.has_target = True\n",
    "        else:\n",
    "            self.has_target = False\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.has_target:\n",
    "            return self.X[idx], self.y[idx]\n",
    "        else:\n",
    "            return self.X[idx]\n",
    "\n",
    "# Définition du modèle de réseau de neurones\n",
    "class CO2RegressionNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(CO2RegressionNet, self).__init__()\n",
    "        \n",
    "        # Architecture plus stable\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            # nn.BatchNorm1d(128),\n",
    "            # nn.Dropout(0.3),  # Ajouter du dropout pour régulariser\n",
    "            \n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            # nn.BatchNorm1d(128),\n",
    "            # nn.Dropout(0.2),  \n",
    "            \n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# Création des datasets et dataloaders\n",
    "train_dataset = CO2Dataset(X_train_nn, y_train_nn)\n",
    "val_dataset = CO2Dataset(X_val, y_val)\n",
    "test_dataset = CO2Dataset(X_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vous avez décidé de ne pas charger de modèle. Initialisation d'un nouveau modèle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luc/INSA/4IF/ML/4IF_ML_TP1/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialisation du modèle, fonction de perte et optimiseur\n",
    "input_dim = X_train.shape[1]\n",
    "model = CO2RegressionNet(input_dim).to(device)\n",
    "\n",
    "try:\n",
    "    # Chargement des poids sauvegardés\n",
    "    nomModel = input(\"Prendre un modèle ? (entrez le nom du fichier en entier ou non pour continuer sans) : \")\n",
    "    if nomModel != \"non\":\n",
    "        model.load_state_dict(torch.load(nomModel + '.pth'))\n",
    "    else:\n",
    "        print(\"Vous avez décidé de ne pas charger de modèle. Initialisation d'un nouveau modèle\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Pas de modèle sauvegardé trouvé, initialisation d'un nouveau modèle.\")\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "        m.bias.data.fill_(0.01)\n",
    "        \n",
    "model.apply(init_weights)\n",
    "\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()  # Mean Squared Error pour la régression\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min',\n",
    "    factor=0.7,       # Réduction plus douce (0.7 au lieu de 0.5)\n",
    "    patience=5,       # Plus de patience (5 au lieu de 3)\n",
    "    threshold=0.01,   # Seuil moins restrictif\n",
    "    min_lr=1e-6,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction d'entraînement\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100, scheduler=None):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Mode entraînement\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            # Remise à zéro des gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            \n",
    "            # Backward pass et optimisation\n",
    "            loss.backward()\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * X_batch.size(0)\n",
    "        \n",
    "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        \n",
    "        # Mode évaluation\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                \n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                \n",
    "                running_loss += loss.item() * X_batch.size(0)\n",
    "        \n",
    "        epoch_val_loss = running_loss / len(val_loader.dataset)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        \n",
    "        # Ajustement du learning rate\n",
    "        if scheduler is not None and active_scheduler:\n",
    "            # Comme c'est un ReduceLROnPlateau, nous lui passons la perte de validation\n",
    "            scheduler.step(epoch_val_loss)\n",
    "        \n",
    "        # Récupération du learning rate actuel pour l'affichage\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Affichage des métriques\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}, LR: {current_lr:.6f}')\n",
    "        \n",
    "        # Sauvegarde du meilleur modèle\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "    \n",
    "    # Chargement du meilleur modèle\n",
    "    model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Démarage de l'entraînement avec les paramètres suivants:\n",
      "-\t Learning Rate: 0.001\n",
      "-\t Weight Decay: 1e-05\n",
      "-\t Active Scheduler: False\n",
      "-\t Batch Size: 64\n",
      "\n",
      "\n",
      "Epoch 1/100, Train Loss: 4127.3187, Val Loss: 48.1855, LR: 0.001000\n",
      "Epoch 2/100, Train Loss: 16.6860, Val Loss: 29.6775, LR: 0.001000\n",
      "Epoch 3/100, Train Loss: 6.0468, Val Loss: 23.6951, LR: 0.001000\n",
      "Epoch 4/100, Train Loss: 2.8338, Val Loss: 21.8772, LR: 0.001000\n",
      "Epoch 5/100, Train Loss: 1.7268, Val Loss: 20.5640, LR: 0.001000\n",
      "Epoch 6/100, Train Loss: 1.1653, Val Loss: 19.4495, LR: 0.001000\n",
      "Epoch 7/100, Train Loss: 0.9169, Val Loss: 19.4314, LR: 0.001000\n",
      "Epoch 8/100, Train Loss: 0.7087, Val Loss: 18.2100, LR: 0.001000\n",
      "Epoch 9/100, Train Loss: 0.6264, Val Loss: 18.1903, LR: 0.001000\n",
      "Epoch 10/100, Train Loss: 0.6051, Val Loss: 17.6040, LR: 0.001000\n",
      "Epoch 11/100, Train Loss: 0.5815, Val Loss: 17.3369, LR: 0.001000\n",
      "Epoch 12/100, Train Loss: 0.5896, Val Loss: 16.1072, LR: 0.001000\n",
      "Epoch 13/100, Train Loss: 0.5479, Val Loss: 16.2845, LR: 0.001000\n",
      "Epoch 14/100, Train Loss: 0.5494, Val Loss: 15.7152, LR: 0.001000\n",
      "Epoch 15/100, Train Loss: 0.5299, Val Loss: 15.9418, LR: 0.001000\n"
     ]
    }
   ],
   "source": [
    "# Entraînement du modèle\n",
    "\n",
    "print(\"Démarage de l'entraînement avec les paramètres suivants:\")\n",
    "print(f\"-\\t Learning Rate: {LR}\")\n",
    "print(f\"-\\t Weight Decay: {WEIGHT_DECAY}\")\n",
    "print(f\"-\\t Active Scheduler: {active_scheduler}\")\n",
    "print(f\"-\\t Batch Size: {BATCH_SIZE}\")\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "trained_model, train_losses, val_losses = train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, num_epochs=NUM_EPOCHS, scheduler=scheduler\n",
    ")\n",
    "\n",
    "torch.save(trained_model.state_dict(), 'co2_regression_model.pth')\n",
    "print(\"Modèle sauvegardé dans 'co2_regression_model.pth'\")\n",
    "\n",
    "# Graphique des pertes\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Courbes d\\'apprentissage')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('nn_training_loss.png')\n",
    "plt.close()\n",
    "\n",
    "# Évaluation sur l'ensemble de validation\n",
    "model.eval()\n",
    "val_predictions = []\n",
    "val_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in val_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        val_predictions.extend(outputs.cpu().numpy())\n",
    "        val_targets.extend(y_batch.numpy())\n",
    "\n",
    "val_predictions = np.array(val_predictions).flatten()\n",
    "val_targets = np.array(val_targets).flatten()\n",
    "\n",
    "val_mae = mean_absolute_error(val_targets, val_predictions)\n",
    "val_r2 = r2_score(val_targets, val_predictions)\n",
    "\n",
    "print(f\"Validation MAE: {val_mae:.4f}\")\n",
    "print(f\"Validation R²: {val_r2:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Prédictions sur l'ensemble de test\n",
    "model.eval()\n",
    "test_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        test_predictions.extend(outputs.cpu().numpy())\n",
    "\n",
    "Y_test_nn = np.array(test_predictions).flatten()\n",
    "\n",
    "# Sauvegarde des résultats\n",
    "resultat = [(test_df['id'].iloc[i], Y_test_nn[i]) for i in range(len(Y_test_nn))]\n",
    "with open('resultat_pytorch.csv', 'w') as f:\n",
    "    f.write(\"id,co2\\n\")\n",
    "    for id, co2 in resultat:\n",
    "        f.write(f\"{id},{co2}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
