{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des bibliothèques nécessaires\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, make_scorer, mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "\n",
    "\n",
    "# Sélection des features\n",
    "numeric_features = ['max_power', 'weight_min', 'weight_max', 'urb_cons', \n",
    "                    'exturb_cons', 'overall_cons', 'co', 'hc', 'nox', 'hcnox', 'ptcl']\n",
    "categorical_features = [ 'brand', 'range', 'grbx_type_ratios', 'car_class', 'fuel_type', 'hybrid']\n",
    "\n",
    "# Créer une copie pour éviter de modifier les données originales\n",
    "X_train = train_df.copy()\n",
    "X_test = test_df.copy()\n",
    "\n",
    "# Dropping things:\n",
    "# Model:\n",
    "X_train.drop('model', axis=1, inplace=True)\n",
    "X_test.drop('model', axis=1, inplace=True)\n",
    "# ID:\n",
    "X_train = X_train.drop('id', axis=1)\n",
    "X_test = X_test.drop('id', axis=1)\n",
    "\n",
    "\n",
    "# # Brand:\n",
    "# X_train = X_train.drop('brand', axis=1)\n",
    "# X_test = X_test.drop('brand', axis=1)\n",
    "# # grbx_type_ratios:\n",
    "# X_train = X_train.drop('grbx_type_ratios', axis=1)\n",
    "# X_test = X_test.drop('grbx_type_ratios', axis=1)\n",
    "# # range:\n",
    "# X_train = X_train.drop('range', axis=1)\n",
    "# X_test = X_test.drop('range', axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forme des données d'entraînement finales: (41257, 102)\n",
      "Forme des données de test finales: (13753, 102)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41257 entries, 0 to 41256\n",
      "Columns: 102 entries, max_power to hybrid_oui\n",
      "dtypes: float64(102)\n",
      "memory usage: 32.1 MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>brand</th>\n",
       "      <th>model</th>\n",
       "      <th>car_class</th>\n",
       "      <th>range</th>\n",
       "      <th>fuel_type</th>\n",
       "      <th>hybrid</th>\n",
       "      <th>max_power</th>\n",
       "      <th>grbx_type_ratios</th>\n",
       "      <th>weight_min</th>\n",
       "      <th>weight_max</th>\n",
       "      <th>urb_cons</th>\n",
       "      <th>exturb_cons</th>\n",
       "      <th>overall_cons</th>\n",
       "      <th>co</th>\n",
       "      <th>hc</th>\n",
       "      <th>nox</th>\n",
       "      <th>hcnox</th>\n",
       "      <th>ptcl</th>\n",
       "      <th>co2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>MERCEDES</td>\n",
       "      <td>COMBI 110 CDI</td>\n",
       "      <td>MINIBUS</td>\n",
       "      <td>MOY-INFER</td>\n",
       "      <td>GO</td>\n",
       "      <td>non</td>\n",
       "      <td>70.0</td>\n",
       "      <td>M 6</td>\n",
       "      <td>1976</td>\n",
       "      <td>2075</td>\n",
       "      <td>9.1</td>\n",
       "      <td>6.4</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.083</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.001</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>MERCEDES</td>\n",
       "      <td>VIANO 2.0 CDI</td>\n",
       "      <td>MINIBUS</td>\n",
       "      <td>MOY-SUPER</td>\n",
       "      <td>GO</td>\n",
       "      <td>non</td>\n",
       "      <td>100.0</td>\n",
       "      <td>A 5</td>\n",
       "      <td>2186</td>\n",
       "      <td>2355</td>\n",
       "      <td>10.2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.2</td>\n",
       "      <td>0.078</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.001</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>MERCEDES</td>\n",
       "      <td>SPRINTER COMBI 319 CDI</td>\n",
       "      <td>MINIBUS</td>\n",
       "      <td>MOY-INFER</td>\n",
       "      <td>GO</td>\n",
       "      <td>non</td>\n",
       "      <td>140.0</td>\n",
       "      <td>A 5</td>\n",
       "      <td>2586</td>\n",
       "      <td>2869</td>\n",
       "      <td>12.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.014</td>\n",
       "      <td>1.846</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002</td>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>RENAULT</td>\n",
       "      <td>MEGANE Coupé EnergyTCe (115ch) eco2</td>\n",
       "      <td>COUPE</td>\n",
       "      <td>MOY-INFER</td>\n",
       "      <td>ES</td>\n",
       "      <td>non</td>\n",
       "      <td>85.0</td>\n",
       "      <td>M 6</td>\n",
       "      <td>1280</td>\n",
       "      <td>1280</td>\n",
       "      <td>6.4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>5.3</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>MERCEDES</td>\n",
       "      <td>COMBI 116 CDI</td>\n",
       "      <td>MINIBUS</td>\n",
       "      <td>MOY-INFER</td>\n",
       "      <td>GO</td>\n",
       "      <td>non</td>\n",
       "      <td>120.0</td>\n",
       "      <td>A 5</td>\n",
       "      <td>2356</td>\n",
       "      <td>2450</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6.9</td>\n",
       "      <td>8.1</td>\n",
       "      <td>0.042</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.001</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     brand                                model car_class      range  \\\n",
       "0   0  MERCEDES                        COMBI 110 CDI   MINIBUS  MOY-INFER   \n",
       "1   1  MERCEDES                        VIANO 2.0 CDI   MINIBUS  MOY-SUPER   \n",
       "2   2  MERCEDES               SPRINTER COMBI 319 CDI   MINIBUS  MOY-INFER   \n",
       "3   3   RENAULT  MEGANE Coupé EnergyTCe (115ch) eco2     COUPE  MOY-INFER   \n",
       "4   4  MERCEDES                        COMBI 116 CDI   MINIBUS  MOY-INFER   \n",
       "\n",
       "  fuel_type hybrid  max_power grbx_type_ratios  weight_min  weight_max  \\\n",
       "0        GO    non       70.0              M 6        1976        2075   \n",
       "1        GO    non      100.0              A 5        2186        2355   \n",
       "2        GO    non      140.0              A 5        2586        2869   \n",
       "3        ES    non       85.0              M 6        1280        1280   \n",
       "4        GO    non      120.0              A 5        2356        2450   \n",
       "\n",
       "   urb_cons  exturb_cons  overall_cons     co     hc    nox  hcnox   ptcl  co2  \n",
       "0       9.1          6.4           7.4  0.083    NaN  0.229  0.250  0.001  195  \n",
       "1      10.2          7.0           8.2  0.078    NaN  0.224  0.233  0.001  216  \n",
       "2      12.5          9.0          10.3  0.067  0.014  1.846    NaN  0.002  272  \n",
       "3       6.4          4.6           5.3  0.167  0.039  0.039    NaN  0.001  119  \n",
       "4      10.1          6.9           8.1  0.042    NaN  0.190  0.201  0.001  214  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 1. Gestion des valeurs manquantes et standardisation des variables numériques\n",
    "for col in numeric_features:\n",
    "    combined_values = pd.concat([X_train[col], X_test[col]])\n",
    "    mean_value = combined_values.mean()\n",
    "    std_value = combined_values.std()\n",
    "    \n",
    "    X_train[col] = X_train[col].fillna(mean_value)\n",
    "    X_test[col] = X_test[col].fillna(mean_value)\n",
    "    \n",
    "    X_train[col] = (X_train[col] - mean_value) / std_value\n",
    "    X_test[col] = (X_test[col] - mean_value) / std_value\n",
    "\n",
    "#Standardisation de co2 et création de y_train\n",
    "co2_mean = X_train['co2'].mean()\n",
    "co2_std = X_train['co2'].std()\n",
    "X_train['co2'] = (X_train['co2'] - co2_mean) / co2_std\n",
    "y_train = X_train.pop('co2')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2. Encodage des variables catégoriques avec OneHotEncoder\n",
    "\n",
    "# Initialisation du OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit sur les données d'entraînement uniquement\n",
    "encoder.fit(X_train[categorical_features])\n",
    "\n",
    "# Transformation des données d'entraînement et de test\n",
    "encoded_train = encoder.transform(X_train[categorical_features])\n",
    "encoded_test = encoder.transform(X_test[categorical_features])\n",
    "\n",
    "# Création de DataFrame à partir des données encodées avec les noms des catégories\n",
    "encoded_feature_names = []\n",
    "for i, feature in enumerate(categorical_features):\n",
    "    categories = encoder.categories_[i]\n",
    "    for category in categories:\n",
    "        encoded_feature_names.append(f\"{feature}_{category}\")\n",
    "    \n",
    "\n",
    "# Conversion des données encodées en DataFrame\n",
    "encoded_train_df = pd.DataFrame(encoded_train, columns=encoded_feature_names, index=X_train.index)\n",
    "encoded_test_df = pd.DataFrame(encoded_test, columns=encoded_feature_names, index=X_test.index)\n",
    "\n",
    "# Suppression des colonnes catégoriques originales et ajout des colonnes encodées\n",
    "X_train_numeric = X_train.drop(categorical_features, axis=1)\n",
    "X_test_numeric = X_test.drop(categorical_features, axis=1)\n",
    "\n",
    "# Concaténation des variables numériques et catégoriques encodées\n",
    "X_train = pd.concat([X_train_numeric, encoded_train_df], axis=1)\n",
    "X_test = pd.concat([X_test_numeric, encoded_test_df], axis=1)\n",
    "\n",
    "# Vérification du résultat\n",
    "print(f\"Forme des données d'entraînement finales: {X_train.shape}\")\n",
    "print(f\"Forme des données de test finales: {X_test.shape}\")\n",
    "\n",
    "print(X_train.info())\n",
    "train_df.head()\n",
    "\n",
    "\n",
    "# 2. Encodage des variables catégoriques avec LabelEncoder au lieu de OneHotEncoder\n",
    "\n",
    "# # Initialisation des LabelEncoders pour chaque feature catégorique\n",
    "# label_encoders = {}\n",
    "# for feature in categorical_features:\n",
    "#     label_encoders[feature] = LabelEncoder()\n",
    "    \n",
    "#     # Fit sur les données d'entraînement uniquement\n",
    "#     X_train[feature] = label_encoders[feature].fit_transform(X_train[feature].fillna('unknown'))\n",
    "    \n",
    "#     # Transformation des données de test\n",
    "#     # Gérer les catégories inconnues dans l'ensemble de test\n",
    "#     X_test[feature] = X_test[feature].fillna('unknown')\n",
    "#     # Pour gérer les valeurs dans X_test qui n'existent pas dans X_train\n",
    "#     for val in np.unique(X_test[feature]):\n",
    "#         if val not in label_encoders[feature].classes_:\n",
    "#             # Ajouter cette classe à l'encodeur\n",
    "#             label_encoders[feature].classes_ = np.append(label_encoders[feature].classes_, val)\n",
    "    \n",
    "#     X_test[feature] = label_encoders[feature].transform(X_test[feature])\n",
    "\n",
    "# # Assurez-vous que toutes les colonnes sont numériques\n",
    "# X_train = X_train.astype(float)\n",
    "# X_test = X_test.astype(float)\n",
    "\n",
    "# # Vérification du résultat\n",
    "# print(f\"Forme des données d'entraînement finales: {X_train.shape}\")\n",
    "# print(f\"Forme des données de test finales: {X_test.shape}\")\n",
    "\n",
    "# X_train = X_train.drop('id', axis=1)\n",
    "# X_test = X_test.drop('id', axis=1)\n",
    "\n",
    "# print(X_train.info())\n",
    "# X_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "co2                             1.000\n",
      "overall_cons                    0.971\n",
      "exturb_cons                     0.966\n",
      "urb_cons                        0.907\n",
      "weight_min                      0.641\n",
      "car_class_MINIBUS               0.585\n",
      "weight_max                      0.539\n",
      "car_class_BERLINE               0.520\n",
      "grbx_type_ratios_A 5            0.454\n",
      "hcnox                           0.437\n",
      "max_power                       0.346\n",
      "range_INFERIEURE                0.316\n",
      "grbx_type_ratios_M 5            0.290\n",
      "range_SUPERIEURE                0.248\n",
      "brand_MERCEDES                  0.247\n",
      "nox                             0.243\n",
      "ptcl                            0.201\n",
      "grbx_type_ratios_M 6            0.201\n",
      "car_class_BREAK                 0.198\n",
      "brand_FIAT                      0.175\n",
      "hybrid_non                      0.174\n",
      "hybrid_oui                      0.174\n",
      "brand_OPEL                      0.174\n",
      "range_ECONOMIQUE                0.171\n",
      "car_class_MONOSPACE COMPACT     0.170\n",
      "hc                              0.164\n",
      "grbx_type_ratios_V 0            0.153\n",
      "range_MOY-INFER                 0.144\n",
      "co                              0.143\n",
      "fuel_type_EH                    0.140\n",
      "brand_BMW                       0.138\n",
      "brand_ASTON MARTIN              0.137\n",
      "brand_CITROEN                   0.133\n",
      "brand_ALFA-ROMEO                0.125\n",
      "range_MOY-SUPER                 0.124\n",
      "car_class_MINISPACE             0.118\n",
      "range_LUXE                      0.117\n",
      "car_class_COMBISPACE            0.116\n",
      "brand_AUDI                      0.116\n",
      "brand_PEUGEOT                   0.111\n",
      "brand_SKODA                     0.108\n",
      "brand_RENAULT                   0.105\n",
      "brand_MINI                      0.104\n",
      "brand_LAMBORGHINI               0.102\n",
      "brand_SEAT                      0.101\n",
      "grbx_type_ratios_A 7            0.099\n",
      "fuel_type_GH                    0.094\n",
      "brand_TOYOTA                    0.094\n",
      "car_class_TS TERRAINS/CHEMINS   0.092\n",
      "brand_FORD                      0.088\n",
      "grbx_type_ratios_D 5            0.081\n",
      "brand_VOLVO                     0.081\n",
      "brand_ROLLS-ROYCE               0.077\n",
      "brand_SMART                     0.073\n",
      "brand_KIA                       0.072\n",
      "car_class_CABRIOLET             0.068\n",
      "brand_HYUNDAI                   0.066\n",
      "grbx_type_ratios_D 7            0.064\n",
      "brand_FERRARI                   0.064\n",
      "brand_HONDA                     0.060\n",
      "brand_LANCIA                    0.059\n",
      "grbx_type_ratios_A 6            0.056\n",
      "fuel_type_EE                    0.054\n",
      "brand_DACIA                     0.054\n",
      "brand_MAZDA                     0.053\n",
      "brand_BENTLEY                   0.052\n",
      "grbx_type_ratios_D 6            0.050\n",
      "fuel_type_GP/ES                 0.048\n",
      "brand_SUZUKI                    0.046\n",
      "brand_NISSAN                    0.043\n",
      "fuel_type_GO                    0.042\n",
      "brand_LEXUS                     0.042\n",
      "brand_MITSUBISHI                0.041\n",
      "car_class_MONOSPACE             0.041\n",
      "grbx_type_ratios_A 9            0.037\n",
      "fuel_type_ES/GP                 0.036\n",
      "grbx_type_ratios_A 4            0.035\n",
      "brand_CADILLAC                  0.034\n",
      "brand_MASERATI                  0.034\n",
      "fuel_type_GN/ES                 0.033\n",
      "brand_JEEP                      0.030\n",
      "grbx_type_ratios_A 8            0.027\n",
      "brand_SUBARU                    0.024\n",
      "fuel_type_GL                    0.022\n",
      "brand_CHEVROLET                 0.021\n",
      "car_class_COUPE                 0.019\n",
      "fuel_type_ES                    0.018\n",
      "car_class_COMBISPCACE           0.017\n",
      "fuel_type_GN                    0.015\n",
      "brand_JAGUAR                    0.014\n",
      "grbx_type_ratios_M 7            0.014\n",
      "brand_LAND ROVER                0.012\n",
      "range_MOY-INFERIEURE            0.010\n",
      "brand_PORSCHE                   0.009\n",
      "brand_LOTUS                     0.008\n",
      "grbx_type_ratios_V .            0.006\n",
      "brand_SSANGYONG                 0.006\n",
      "brand_LADA                      0.005\n",
      "fuel_type_ES/GN                 0.005\n",
      "brand_VOLKSWAGEN                0.003\n",
      "fuel_type_FE                    0.002\n",
      "grbx_type_ratios_S 6            0.002\n",
      "brand_INFINITI                  0.001\n",
      "Name: co2, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Configurer pandas pour afficher toutes les lignes et colonnes\n",
    "pd.set_option('display.max_rows', None)      # Afficher toutes les lignes\n",
    "pd.set_option('display.max_columns', None)   # Afficher toutes les colonnes\n",
    "pd.set_option('display.width', None)         # Largeur d'affichage automatique\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)  # Limiter à 3 décimales pour la lisibilité\n",
    "\n",
    "# Calculer la matrice de corrélation\n",
    "corr_matrix = (pd.concat([X_train, y_train], axis=1)).corr().abs()\n",
    "\n",
    "# Visualiser les corrélations avec la variable cible\n",
    "co2_correlations = corr_matrix['co2'].sort_values(ascending=False)\n",
    "print(co2_correlations)\n",
    "\n",
    "# Si vous voulez revenir aux paramètres par défaut après l'affichage\n",
    "pd.reset_option('display.max_rows')\n",
    "pd.reset_option('display.max_columns')\n",
    "pd.reset_option('display.width')\n",
    "pd.reset_option('display.float_format')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train_nn, X_val, y_train_nn, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 64\n",
    "LR = 1e-5\n",
    "WEIGHT_DECAY = 1e-5\n",
    "active_scheduler = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilisation de: cpu\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Réseau de neurones avec PyTorch pour la régression:\n",
    "\"\"\"\n",
    "\n",
    "# Vérifier si CUDA est disponible\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Utilisation de: {device}\")\n",
    "\n",
    "# Créer un dataset PyTorch personnalisé\n",
    "class CO2Dataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = torch.tensor(X.values, dtype=torch.float32)\n",
    "        \n",
    "        if y is not None:\n",
    "            self.y = torch.tensor(y.values, dtype=torch.float32).reshape(-1, 1)\n",
    "            self.has_target = True\n",
    "        else:\n",
    "            self.has_target = False\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.has_target:\n",
    "            return self.X[idx], self.y[idx]\n",
    "        else:\n",
    "            return self.X[idx]\n",
    "\n",
    "# Définition du modèle de réseau de neurones\n",
    "class CO2RegressionNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(CO2RegressionNet, self).__init__()\n",
    "        \n",
    "        # Architecture plus stable\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            \n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def __str__(self):\n",
    "        string = \"Description du network utilisé:\\n\"\n",
    "        for i, layer in enumerate(self.model):\n",
    "            string += f\"Layer {i}: {layer}\\n\"\n",
    "        return string\n",
    "    \n",
    "\n",
    "\n",
    "# Création des datasets et dataloaders\n",
    "train_dataset = CO2Dataset(X_train_nn, y_train_nn)\n",
    "val_dataset = CO2Dataset(X_val, y_val)\n",
    "test_dataset = CO2Dataset(X_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vous avez décidé de ne pas charger de modèle. Initialisation d'un nouveau modèle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialisation du modèle, fonction de perte et optimiseur\n",
    "input_dim = X_train.shape[1]\n",
    "model = CO2RegressionNet(input_dim)\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "\n",
    "try:\n",
    "    # Chargement des poids sauvegardés\n",
    "    nomModel = input(\"Prendre un modèle ? (entrez le nom du fichier en entier ou non pour continuer sans) : \")\n",
    "    if nomModel != \"non\":\n",
    "        print(\"Chargement du modèle\", nomModel+\".pth\")\n",
    "        model.load_state_dict(torch.load(nomModel + '.pth'))\n",
    "        \n",
    "    else:\n",
    "        model.apply(init_weights)\n",
    "        print(\"Vous avez décidé de ne pas charger de modèle. Initialisation d'un nouveau modèle\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    model.apply(init_weights)\n",
    "    print(\"Pas de modèle sauvegardé trouvé, initialisation d'un nouveau modèle.\")\n",
    "\n",
    "\n",
    "        \n",
    "model.to(device)\n",
    "\n",
    "\n",
    "criterion = nn.L1Loss()  # Mean Squared Error pour la régression\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min',\n",
    "    factor=0.85,       \n",
    "    patience=3,\n",
    "    min_lr=1e-6,\n",
    "    threshold=0.01,     \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction d'entraînement\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100, scheduler=None):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Mode entraînement\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            # Remise à zéro des gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            \n",
    "            # Backward pass et optimisation\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip gradients\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * X_batch.size(0)\n",
    "        \n",
    "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        \n",
    "        # Mode évaluation\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                \n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                \n",
    "                running_loss += loss.item() * X_batch.size(0)\n",
    "        \n",
    "        epoch_val_loss = running_loss / len(val_loader.dataset)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        \n",
    "        # Ajustement du learning rate\n",
    "        if scheduler is not None and active_scheduler:\n",
    "            # Comme c'est un ReduceLROnPlateau, nous lui passons la perte de validation\n",
    "            scheduler.step(epoch_val_loss)\n",
    "        \n",
    "        # Récupération du learning rate actuel pour l'affichage\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Affichage des métriques\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_train_loss:.6f}, Val Loss: {epoch_val_loss:.6f}, LR: {current_lr:.6f}')\n",
    "        \n",
    "        # Sauvegarde du meilleur modèle\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "    \n",
    "    # Chargement du meilleur modèle\n",
    "    model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Démarage de l'entraînement avec les paramètres suivants:\n",
      "-\t Learning Rate: 1e-05\n",
      "-\t Weight Decay: 1e-05\n",
      "-\t Active Scheduler: True\n",
      "-\t Batch Size: 64\n",
      "-\t Raw CO2 - Min: 13, Max: 572, Mean: 201.6360859975277, Std: 33.9065497455555\n",
      "\n",
      "Réseau de neurones: \n",
      "Description du network utilisé:\n",
      "Layer 0: Linear(in_features=102, out_features=128, bias=True)\n",
      "Layer 1: ReLU()\n",
      "Layer 2: Linear(in_features=128, out_features=64, bias=True)\n",
      "Layer 3: ReLU()\n",
      "Layer 4: Linear(in_features=128, out_features=1, bias=True)\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x64 and 128x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[125]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRéseau de neurones: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m trained_model, train_losses, val_losses = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m torch.save(trained_model.state_dict(), \u001b[33m'\u001b[39m\u001b[33mco2_regression_model.pth\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mModèle sauvegardé dans \u001b[39m\u001b[33m'\u001b[39m\u001b[33mco2_regression_model.pth\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[124]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, scheduler)\u001b[39m\n\u001b[32m     18\u001b[39m optimizer.zero_grad()\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m loss = criterion(outputs, y_batch)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Backward pass et optimisation\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/INSA/4IF/ML/4IF_ML_TP1/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/INSA/4IF/ML/4IF_ML_TP1/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[122]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36mCO2RegressionNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/INSA/4IF/ML/4IF_ML_TP1/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/INSA/4IF/ML/4IF_ML_TP1/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/INSA/4IF/ML/4IF_ML_TP1/.venv/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/INSA/4IF/ML/4IF_ML_TP1/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/INSA/4IF/ML/4IF_ML_TP1/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/INSA/4IF/ML/4IF_ML_TP1/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (64x64 and 128x1)"
     ]
    }
   ],
   "source": [
    "# Entraînement du modèle\n",
    "\n",
    "print(\"Démarage de l'entraînement avec les paramètres suivants:\")\n",
    "print(f\"-\\t Learning Rate: {LR}\")\n",
    "print(f\"-\\t Weight Decay: {WEIGHT_DECAY}\")\n",
    "print(f\"-\\t Active Scheduler: {active_scheduler}\")\n",
    "print(f\"-\\t Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"-\\t Raw CO2 - Min: {train_df['co2'].min()}, Max: {train_df['co2'].max()}, Mean: {co2_mean}, Std: {co2_std}\")\n",
    "print(\"\")\n",
    "print(f\"Réseau de neurones: \\n{model}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "trained_model, train_losses, val_losses = train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, num_epochs=NUM_EPOCHS, scheduler=scheduler\n",
    ")\n",
    "\n",
    "torch.save(trained_model.state_dict(), 'co2_regression_model.pth')\n",
    "print(\"Modèle sauvegardé dans 'co2_regression_model.pth'\")\n",
    "\n",
    "# Graphique des pertes\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Courbes d\\'apprentissage')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('nn_training_loss.png')\n",
    "plt.close()\n",
    "\n",
    "# Évaluation sur l'ensemble de validation\n",
    "model.eval()\n",
    "val_predictions = []\n",
    "val_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in val_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        val_predictions.extend(outputs.cpu().numpy())\n",
    "        val_targets.extend(y_batch.numpy())\n",
    "\n",
    "val_predictions = np.array(val_predictions).flatten()\n",
    "val_predictions = val_predictions * co2_std + co2_mean\n",
    "val_targets = np.array(val_targets).flatten()\n",
    "val_targets = val_targets * co2_std + co2_mean\n",
    "\n",
    "val_mae = mean_absolute_error(val_targets, val_predictions)\n",
    "val_r2 = r2_score(val_targets, val_predictions)\n",
    "\n",
    "print(f\"Validation MAE: {val_mae:.4f}\")\n",
    "print(f\"Validation R²: {val_r2:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Prédictions sur l'ensemble de test\n",
    "model.eval()\n",
    "test_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        test_predictions.extend(outputs.cpu().numpy())\n",
    "\n",
    "Y_test_nn = np.array(test_predictions).flatten()\n",
    "Y_test_nn = Y_test_nn * co2_std + co2_mean\n",
    "\n",
    "# Sauvegarde des résultats\n",
    "resultat = [(test_df['id'].iloc[i], Y_test_nn[i]) for i in range(len(Y_test_nn))]\n",
    "with open('resultat_pytorch.csv', 'w') as f:\n",
    "    f.write(\"id,co2\\n\")\n",
    "    for id, co2 in resultat:\n",
    "        f.write(f\"{id},{co2}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
